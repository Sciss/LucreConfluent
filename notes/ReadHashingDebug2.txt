Traversal of v4 (5)
-------------------

access.listRef @ <0, 1, 2, 4, 5>
(( where the last entry would be <0, 1, 2> ))
; pre = 3
; stored is w0' with <1, 2>
; substitute is <1, 2> ++ <0, 1, 2, 4, 5>.drop(3)
  thus (<1, 2, 4, 5>, f(w0'))

// first of all: we can get around fixAccess
// by including the access path in the flush process!

- let us assume that f(w0') stores an additional
  map for the union of its mutable field modifications
  (head and tail in this case)

// note: collette et al. seem to have a similar idea when they
write "In other words, each object will contain some compacted representation of its own history." (a compressed tree of the versions where a node existed
is stored with the node)

- this would indicate the latest modification is
  at <1, 2> (regarding its tail field)
- let us assume also for the moment that we have a means
  to tell that the tuple should thus be represented as
  (<1, 2> | <4, 5>, f(w0'))

; reading head and tail ok then
; resolving tail ref:

; access is <1, 2>
; stored is w0 with <1, 2>?
; it's modification would indicate <1, 2, 4>

... // todo

:::::::::::::::::::::

Other Idea
----------
- let the nodes always start with a prefix which is from v0
  to the access context
  - we may find a shortcut to storing and retrieving the paths
    so that they are only theoretically longer
    (e.g. just store the suffix, but the hash based on the full
    path)
  - we may solve the problem of tracking read hashes this way
    ( // probably not... )
?

::::::::::::::::::::::

Other Idea
----------
- allow hash collisions
  but how to handle equals efficiently still?

::::::::::::::::::::::

generally we need to be able to match a read access
against a set of write paths (across all objects)
such that we can find the maximum prefix in it and
split the read access into a pre and post part.

--- recall ---

assignment pedigrees (zero based):
0
0, 1
0, 1, 3  // i.e. 1, 2, 4
0, 2
0, 2, 3
2
2, 3
2, 3, 4

missing paths (zero based):
0, 1, 3, 4  // i.e. 1, 2, 4, 5
0, 2, 3, 4
0, 2, 4
2, 4

observation:
- each sequence increases strictly monotonically
- it could thus be represented in a differentiated form:

assignment pedigrees (idx0, then diffs):
0
0, 1
0, 1, 2
0, 2
0, 2, 1
2
2, 1
2, 1, 1

missing paths (idx0, then diffs):
0, 1, 2, 1
0, 1, 1, 1
0, 2, 2
2, 2

-- does that help in any way?
( // no -- in fact, splitting the paths becomes difficult )

--- compressed path rep ---

assignment pedigrees (zero based):
<0, 0>
<0, 1>
<0, 1, 3, 3>
<0, 2>
<0, 2, 3, 3>
<2, 2>
<2, 2, 3, 3>
<2, 2, 3, 4>

missing paths (zero based):
<0, 1, 3, 4>
<0, 2, 3, 4>
<0, 2, 4, 4>
<2, 2, 4, 4>

::::::::::::::::::

Other Idea
----------
pre calculate the 'random' id sequence such that there is guaranteed no collisions in all possible permutations? is that viable? how fast would the hash bit size grow for a given number of steps?

for a sequence of length n, there are (1 << n) - 1 possible paths
(all n bit combinations except 0). -- this idea would be exhausted after a sequence length of 64 :-(

::::::::::::::::::

- we maintain a set of paths per access path, so that the whole set must be considered in the random nextID method
- how will this set grow over time?

recall: missing paths (zero based):
(( note: these all come from the melding ))
0, 2, 3, 4
0, 2, 4
2, 4


v0
  + <0> = [ <0> ]

v1
  + <0, 1> = [ <0>, <0, 1> ]

v2
  ++ [ <0, 2>, <2> ] = [ <0>, <0, 1>, <0, 2>, <2> ]
  
v3
  + <0, 1, 3>
; meld access! --> + <0, 2, 3>  (in this case also produced by the increment)
  + <2, 3>
  = [ <0>, <0, 1>, <0, 2>, <2>, <0, 1, 3>, <0, 2, 3>, <2, 3> ]

v4
  access update: + <0, 1, 3, 4>
  concat: <2, 3, 4>
  from previous set, all elements ending in any of the indegree versions! require
     update, thus, all that end in 2 or 3; these are :
     [ <0, 2>, <2>, <0, 1, 3>, <0, 2, 3>, <2, 3> ]
     which become
     [ <0, 2, 4>, <2, 4>, <0, 1, 3, 4>, <0, 2, 3, 4>, <2, 3, 4> ]
   yielding
   = [ <0>, <0, 1>, <0, 2>, <2>, <0, 1, 3>, <0, 2, 3>, <2, 3>,
       <0, 2, 4>, <2, 4>, <0, 1, 3, 4>, <0, 2, 3, 4>, <2, 3, 4> ] // size = 12

this is the core thought:
"from previous set, all elements ending in any of the indegree versions require updates"

problems:
- needs proof (that there are no collisions possible)
- yields a version build slow down of at least O(N) ?
- although this may be less worse in the compressed path method?
- at least this guarantees that pure read-only work on the
  data structure doesn't suffer from this slowdown

ideas
- each mutable node could have a 'hash offset', e.g. w0'', although
  its seminal node is v2, would have the hash offset corresponding
  to the randomized id of v0. linearity is preserved, since only
  older versions are part of the hash offset, that is to say, it's
  not possible to actually have a read key with <v0, v2> for w0''.
  but this may save quite some space in the sums-taken set?
  ; in the above example, the size would shrink in v4 from 12 elements
  to 8 -- may be worth an experiment with large DAG sizes
  ; in a meld the offset used should be arbitrarily chosen from the
  indegrees (access pointers)?
- a 'garbage collector' could sweep the whole data structure and
  remove unnessary entries -- although maybe there aren't any??
  since the last result (the 12 entries) are all accessed through
  reads when going through v4...
